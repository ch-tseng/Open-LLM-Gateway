import streamlit as st
import requests
import json
import os
import sseclient
import configparser
from pathlib import Path

# è¨­å®šé é¢é…ç½®å’Œæ¨™é¡Œ - å¿…é ˆæ˜¯ç¬¬ä¸€å€‹Streamlitå‘½ä»¤
st.set_page_config(
    page_title="å¤šæ¨¡å‹LLMèŠå¤©ç•Œé¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

# é»˜èªæ¨¡å‹è¨­å®š - ç¡¬ç·¨ç¢¼æ‰€æœ‰æ¨¡å‹
DEFAULT_MODELS = {
    "openai": [ "gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano", "o4", "o3", "o3-mini", "o1", "o1-pro"],
    "claude": ["claude-3-7-sonnet-latest", "claude-3-5-haiku-latest", "claude-3-5-sonnet-latest", "claude-3-opus-latest"],
    "gemini": ["gemini-2.0-flash", "gemini-2.0-flash-lite", "gemini-1.5-flash", "gemini-2.5-flash-preview-04-17", "gemini-2.5-pro-preview-05-06", "gemini-1.0-pro"],
    "deepseek": ["deepseek-chat"],
    "minmax": ["minimax-text-01", "abab6.5s-chat"],
    "ollama": ["qwen3:8b", "gemma3:12b"],
    "huggingface": ["meta-llama/Llama-3-8b-chat", "meta-llama/Llama-3-70b-chat", "mistralai/Mistral-7B-Instruct-v0.2", "google/gemma-7b-it", "microsoft/phi-3-medium-4k-instruct"]
}

DEFAULT_API_ENDPOINT = "http://172.30.11.15:8000"
API_KEY = "AT0130-20250508-kbxk8c"

# è¼‰å…¥é…ç½® - ç°¡åŒ–ç‚ºç›´æ¥ä½¿ç”¨é»˜èªå€¼
MODELS = DEFAULT_MODELS
DEFAULT_ENDPOINT = DEFAULT_API_ENDPOINT

# å˜—è©¦å¾models.confè¼‰å…¥è¨­å®šï¼ˆå¦‚æœå­˜åœ¨ï¼‰
config_file = Path(__file__).parent / "models.conf"
if config_file.exists():
    try:
        # å˜—è©¦ä½¿ç”¨ConfigParserè®€å–é…ç½®ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼‰
        config = configparser.ConfigParser()
        try:
            config.read(config_file, encoding='utf-8')
        except:
            pass  # å¿½ç•¥è®€å–éŒ¯èª¤ï¼Œä½¿ç”¨é»˜èªå€¼
        
        # å¦‚æœæˆåŠŸè®€å–äº†generalå€æ®µï¼Œå˜—è©¦ç²å–APIç«¯é»
        if 'general' in config and 'default_api_endpoint' in config['general']:
            DEFAULT_ENDPOINT = config['general']['default_api_endpoint']
            
        st.sidebar.success("æˆåŠŸè¼‰å…¥APIç«¯é»è¨­å®š")
    except Exception:
        pass  # å¿½ç•¥ä»»ä½•éŒ¯èª¤ï¼Œä½¿ç”¨é»˜èªå€¼
else:
    st.sidebar.info("æœªæ‰¾åˆ°models.confé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜èªè¨­å®š")

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'api_endpoint' not in st.session_state:
    st.session_state.api_endpoint = DEFAULT_ENDPOINT

if 'api_key' not in st.session_state:
    st.session_state.api_key = API_KEY

if 'selected_vendor' not in st.session_state:
    st.session_state.selected_vendor = "openai"

if 'selected_model' not in st.session_state:
    st.session_state.selected_model = MODELS["openai"][0]

# è¨­å®šå´é‚Šæ¬„
with st.sidebar:
    st.title("ğŸ¤– å¤šæ¨¡å‹LLMèŠå¤©")
    
    # APIç«¯é»è¨­å®š
    st.subheader("APIç«¯é»è¨­å®š")
    api_endpoint = st.text_input(
        "LLMä¸»æ©Ÿ (IP:port)",
        value=st.session_state.api_endpoint
    )
    
    # æ›´æ–°APIç«¯é»
    if api_endpoint != st.session_state.api_endpoint:
        st.session_state.api_endpoint = api_endpoint
    
    # APIé‡‘é‘°è¨­å®š
    api_key = st.text_input(
        "APIé‡‘é‘°",
        value=st.session_state.api_key,
        type="password"
    )
    
    # æ›´æ–°APIé‡‘é‘°
    if api_key != st.session_state.api_key:
        st.session_state.api_key = api_key
    
    # LLMä¾›æ‡‰å•†é¸æ“‡
    st.subheader("LLMä¾›æ‡‰å•†é¸æ“‡")
    vendor = st.selectbox(
        "é¸æ“‡LLMä¾›æ‡‰å•†",
        options=["openai", "claude", "gemini", "deepseek", "minmax", "ollama", "huggingface"],
        index=["openai", "claude", "gemini", "deepseek", "minmax", "ollama", "huggingface"].index(st.session_state.selected_vendor)
    )
    
    # æ›´æ–°ä¾›æ‡‰å•†é¸æ“‡
    if vendor != st.session_state.selected_vendor:
        st.session_state.selected_vendor = vendor
        # ç•¶åˆ‡æ›ä¾›æ‡‰å•†æ™‚ï¼Œé¸æ“‡è©²ä¾›æ‡‰å•†çš„ç¬¬ä¸€å€‹æ¨¡å‹
        st.session_state.selected_model = MODELS[vendor][0]
    
    # æ¨¡å‹é¸æ“‡ï¼ˆä¸‹æ‹‰é¸å–®ï¼‰
    available_models = MODELS[vendor]
    if st.session_state.selected_model in available_models:
        model_index = available_models.index(st.session_state.selected_model)
    else:
        model_index = 0
    
    model = st.selectbox(
        f"é¸æ“‡{vendor}æ¨¡å‹",
        options=available_models,
        index=model_index
    )
    
    # æ›´æ–°æ¨¡å‹é¸æ“‡
    if model != st.session_state.selected_model:
        st.session_state.selected_model = model
    
    # ä¸²æµé¸é …
    stream_enabled = st.checkbox("å•Ÿç”¨ä¸²æµå›æ‡‰", value=True)
    
    # æ¸…é™¤å°è©±é¸é …
    if st.button("æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    st.caption("Â© 2024 LLMç¶²é—œèŠå¤©ç•Œé¢")

# ä¸»è¦èŠå¤©ç•Œé¢
st.title("ğŸ’¬ èŠå¤©ç•Œé¢")

# é¡¯ç¤ºèŠå¤©è¨Šæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# å‡½æ•¸ï¼šç™¼é€éä¸²æµè«‹æ±‚
def send_non_stream_request(prompt):
    api_url = f"{st.session_state.api_endpoint}/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {st.session_state.api_key}"
    }
    
    # æ§‹å»ºå®Œæ•´æ¨¡å‹åç¨±ï¼ˆåŠ ä¸Šä¾›æ‡‰å•†å‰ç¶´ï¼‰
    full_model_name = f"{st.session_state.selected_vendor}/{st.session_state.selected_model}"
    
    # æ§‹å»ºè«‹æ±‚æ•¸æ“š
    data = {
        "model": full_model_name,
        "messages": [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
        "temperature": 0.7,
        "stream": False
    }
    
    try:
        with st.spinner("AIæ€è€ƒä¸­..."):
            response = requests.post(api_url, headers=headers, data=json.dumps(data), timeout=90)
            response.raise_for_status()
            result = response.json()
            
            if result.get('choices') and len(result['choices']) > 0:
                assistant_message = result['choices'][0]['message']['content']
                return assistant_message
            else:
                return "ç„¡æ³•ç²å–æœ‰æ•ˆå›æ‡‰ã€‚è«‹æª¢æŸ¥APIè¨­å®šæˆ–ç¨å¾Œå†è©¦ã€‚"
    
    except requests.exceptions.RequestException as e:
        error_message = f"è«‹æ±‚éŒ¯èª¤: {str(e)}"
        if hasattr(e, 'response') and e.response:
            try:
                error_detail = e.response.json()
                return f"éŒ¯èª¤è©³æƒ…: {json.dumps(error_detail)}"
            except:
                return f"éŒ¯èª¤ç‹€æ…‹ç¢¼: {e.response.status_code}, éŒ¯èª¤å…§å®¹: {e.response.text}"
        return error_message

# å‡½æ•¸ï¼šç™¼é€ä¸²æµè«‹æ±‚
def send_stream_request(prompt):
    api_url = f"{st.session_state.api_endpoint}/v1/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {st.session_state.api_key}"
    }
    
    # æ§‹å»ºå®Œæ•´æ¨¡å‹åç¨±ï¼ˆåŠ ä¸Šä¾›æ‡‰å•†å‰ç¶´ï¼‰
    full_model_name = f"{st.session_state.selected_vendor}/{st.session_state.selected_model}"
    
    # æ§‹å»ºè«‹æ±‚æ•¸æ“š
    data = {
        "model": full_model_name,
        "messages": [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
        "temperature": 0.7,
        "stream": True
    }
    
    try:
        # å‰µå»ºä¸€å€‹ç©ºçš„ä½”ä½å›æ‡‰
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            with requests.post(api_url, headers=headers, data=json.dumps(data), stream=True) as response:
                response.raise_for_status()
                client = sseclient.SSEClient(response)
                
                for event in client.events():
                    if event.data == "[DONE]":
                        break
                    
                    try:
                        chunk = json.loads(event.data)
                        if chunk.get('choices') and len(chunk['choices']) > 0:
                            delta = chunk['choices'][0].get('delta', {})
                            if 'content' in delta:
                                full_response += delta['content']
                                message_placeholder.markdown(full_response + "â–Œ")
                    except json.JSONDecodeError:
                        continue
                
                # æœ€çµ‚é¡¯ç¤ºå®Œæ•´å›æ‡‰ï¼ˆç„¡é–ƒçˆæ¸¸æ¨™ï¼‰
                message_placeholder.markdown(full_response)
                return full_response
    
    except requests.exceptions.RequestException as e:
        error_message = f"è«‹æ±‚éŒ¯èª¤: {str(e)}"
        if hasattr(e, 'response') and e.response:
            try:
                error_detail = e.response.json()
                return f"éŒ¯èª¤è©³æƒ…: {json.dumps(error_detail)}"
            except:
                return f"éŒ¯èª¤ç‹€æ…‹ç¢¼: {e.response.status_code}, éŒ¯èª¤å…§å®¹: {e.response.text}"
        return error_message

# èŠå¤©è¼¸å…¥
if prompt := st.chat_input("è¼¸å…¥æ‚¨çš„è¨Šæ¯..."):
    # æ·»åŠ ç”¨æˆ¶è¨Šæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
    with st.chat_message("user"):
        st.write(prompt)
    
    # ç²å–ä¸¦é¡¯ç¤ºåŠ©æ‰‹å›æ‡‰
    if stream_enabled:
        assistant_response = send_stream_request(prompt)
    else:
        # é¡¯ç¤ºåŠ©æ‰‹è¨Šæ¯ï¼ˆéä¸²æµæ¨¡å¼ï¼‰
        with st.chat_message("assistant"):
            with st.spinner("AIæ€è€ƒä¸­..."):
                assistant_response = send_non_stream_request(prompt)
                st.write(assistant_response)
    
    # å°‡åŠ©æ‰‹è¨Šæ¯æ·»åŠ åˆ°æœƒè©±æ­·å²
    st.session_state.messages.append({"role": "assistant", "content": assistant_response}) 